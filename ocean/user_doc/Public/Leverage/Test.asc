== Test Leverage



[quote, ~ Anonymous]
____
“If you don’t like testing your product, most likely your customers won’t like to test it either.” 
____

=== Testing - Traditional or Practical

There appears to be a gap between the traditional ideas of testing and quality
assurance and the very specific methods and techniques taught by the advocates
of Test-Driven Development. The classic discussions focus on planning and
building large documents to capture the Test Plan, Risk Analysis, Test
Strategy, Test Tools, Test Cases and Defect Tracking. The primary artifacts
generated are documents and the core testing activities aren't automated.
Large numbers of people are hired to interact with the system.

The other main school of thought is based in the principles of Agile Software
Development. The key ideas here deal with automated testing and unit test
frameworks. These are used throughout the implementation phase and serve to
ensure the quality long after the implementation is complete. I believe that
these concepts are the foundation that every testing effort should build on.

Test-Driven Development, in the form most often taught, assumes that you begin
using the correct testing approach at the beginning of the project. Tests are
built as the product is built. The two parallel infrastructures validate each
other by confirming all of the embedded assumptions. Development with two
parallel structures that are mutually reliant on one another is difficult to
achieve after the fact.

What if you are given a million lines of legacy code? Is it really practical
to tell your stakeholders that your first job is to produce a million lines of
test code so that you can do reliable refactoring? Of course not! We need a
method to apply the principles of TDD to existing systems.

This chapter attempts to fill the gap between the ideals of TDD and the
practical realities of legacy software. We will remain true to the ideals of
rigorous testing while making it easy to add and maintain tests that utilize
existing parts of the system. In order to achieve the maximum leverage, we
need test strategies that can be used effectively for both greenfield
applications and legacy code.


=== Expected Results

Traditional testing techniques have many problems that end up compromising the
overall test effort. In the previous chapter we introduced a new style of
testing that works well for test-driven development. We will continue to build
upon these techniques to show a broad range of testing strategies, tactics,
and tools.

The techniques taught here are useful over a broad range of applications and
software types. We will demonstrate how to very quickly create and repair
tests and assemble them into a huge inventory of test cases. Small test shims
can also be added to any type of program to exercise the built-in product
functionality for the purpose of testing. We will examine how to construct
test cases for different types of situations. Finally, we will wrap up with a
look at building an effective testing strategy.


==== Every Test has Output

Our idea of testing is based on every test passing out some type of output.
Some tests will pass out text that represents errors that occur. These tests
are typically silent if everything works correctly. Other tests may pass out
lots of data that is the same each time the test is run.

When a test is run for the first time the output is captured for later. When
the test is rerun, the same output is expected. Each time the test is run the
actual results are compared to the expected output. This ends up creating a
very simple contract for each test.

* run the script
* capture the output
* should be the expected results


==== Unexpected Results

The Unix diff command is used to detect the unexpected results. These
differences are then shown to the tester as a test failure. The assumption is
that the test should produce the same output each time it is run. If not,
there is a mystery to investigate.

In real life the output of many tests contains results that vary each time the
command is run. This means that without additional work put into the test, it
will fail every time. In practice each test starts off noisy and then it is
modified by filtering the output to quiet it down. For example, consider
the following test 'ls -l', which  produces the following output.

    total 72
    drwxr-xr-x@  26 markseaman  staff    884 Apr  6 15:48 Code
    drwx------@  19 markseaman  staff    646 Jul 11 09:18 Dropbox
    drwxr-xr-x@   3 markseaman  staff    102 Feb 21 15:16 Money
    drwxr-xr-x@   9 markseaman  staff    306 Apr  7 11:33 MyBook
    drwxr-xr-x@   2 markseaman  staff     68 Apr 10 07:27 Notebook
    drwxrwxr-x    5 markseaman  staff    170 Jun  7 09:00 logs
    drwxr-xr-x  153 markseaman  staff   5202 Jun 23 08:45 test
<hr><p><a href="StartReading">Read More</a></p><hr>
